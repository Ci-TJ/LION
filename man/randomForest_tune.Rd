% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Modelling.R
\name{randomForest_tune}
\alias{randomForest_tune}
\title{Determine mtry for Random Forest Classifier Using K-Fold Cross Validation}
\usage{
randomForest_tune(
  datasets = list(),
  label.col = 1,
  positive.class = NULL,
  folds.num = 10,
  ntree = 3000,
  mtry.ratios = c(0.1, 0.2, 0.4, 0.6, 0.8),
  seed = 1,
  return.model = TRUE,
  parallel.cores = 2,
  ...
)
}
\arguments{
\item{datasets}{should be a list containing one or several input datasets. If input several datasets, stratified cross validation will be performed. See examples.}

\item{label.col}{an integer. Column number of the label.}

\item{positive.class}{\code{NULL} or string. Which class is the positive class? Should be one
of the classes in label column. The first class in label column will be selected
as the positive class if leave \code{positive.class = NULL}.}

\item{folds.num}{an integer. Number of folds. Default \code{10} for 10-fold cross validation.}

\item{ntree}{integer, number of trees to grow. See \code{\link[randomForest]{randomForest}}.
Default: \code{3000}.}

\item{mtry.ratios}{(only when \code{mode = "retrain"}) used to indicate the ratios of \code{mtry} when tuning the random forest classifier.
\code{mtry} = ratio of mtry * number of features Default: \code{c(0.1, 0.2, 0.4, 0.6, 0.8)}.}

\item{seed}{random seed for data splitting. Integer.}

\item{return.model}{logical. If \code{TRUE}, the function will return a random forest
model built with the optimal \code{ntree}. The training set is the combination of all input datasets.}

\item{parallel.cores}{an integer specifying the number of cores for parallel computation. Default: \code{2}.
Set \code{parallel.cores = -1} to run with all the cores. \code{parallel.cores} should be == -1 or >= 1.}

\item{...}{other parameters (except \code{ntree} and \code{mtry}) passed to \code{\link[randomForest]{randomForest}} function.}
}
\value{
If \code{return.model = TRUR}, the function returns a random forest model.
If \code{FALSE}, the function returns the optimal \code{ntree} and the performance.
}
\description{
Determine mtry for Random Forest Classifier Using K-Fold Cross Validation
}
\examples{

# Following codes only show how to use this function
# and cannot reflect the genuine performance of tools or classifiers.

data(demoPositiveSeq)
data(demoNegativeSeq)

RNA.positive <- demoPositiveSeq$RNA.positive
Pro.positive <- demoPositiveSeq$Pro.positive
RNA.negative <- demoNegativeSeq$RNA.negative
Pro.negative <- demoNegativeSeq$Pro.negative

dataPositive <- featureFreq(seqRNA = RNA.positive, seqPro = Pro.positive,
                            label = "Interact", featureMode = "conc",
                            computePro = "DeNovo", k.Pro = 3, k.RNA = 2,
                            normalize = "none", parallel.cores = 2)

dataNegative <- featureFreq(seqRNA = RNA.negative, seqPro = Pro.negative,
                            label = "Non.Interact", featureMode = "conc",
                            computePro = "DeNovo", k.Pro = 3, k.RNA = 2,
                            normalize = "none", parallel.cores = 2)

dataset <- rbind(dataPositive, dataNegative)

Perf_tune <- randomForest_tune(datasets = list(dataset), label.col = 1,
                               positive.class = "Interact", folds.num = 5,
                               ntree = 150, seed = 123,
                               return.model = TRUE, parallel.cores = 2,
                               importance = TRUE)

# if you have more than one input dataset,
# use "datasets = list(dataset1, dataset2, dataset3)".

}
\seealso{
\code{\link{randomForest_RFE}}, \code{\link{randomForest_CV}}, \code{\link[randomForest]{randomForest}}
}
